#ifndef INTEGRATOR2D_CUHPP
#define INTEGRATOR2D_CUHPP

#include <cuda.h>
#include <cuda_runtime.h>
#include <iostream>
#include "integrator2d.hpp"
#include "fixedtimelqr.cuhpp"

#include "util.cuh"
#define INIT_OBJ_COUNT (1000)

#include "util.h"
// some manual function print to trace execution
// this NDEBUG is from cmake when on {RELEASE or MINSIZEREL} mode
#ifndef NDEBUG
#define TRACE_EXEC
#define TRACE_VAL
#define TRACE_CUDA
#endif

#ifndef gpuErrchk
#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
{
   if (code != cudaSuccess)
   {
      fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
      if (abort) exit(code);
   }
}
#endif

#define WARP_SIZE (32)
#define THREAD_PER_BLOCK (128)

template <int segment = 10, typename State, typename Scalar, typename Input, typename InputMat, typename InputWeight>
__global__
void kernel_solver(State *s0, State *s1, Scalar *time, State *out_state, Input *inputs, InputMat *B, InputWeight *R, size_t n) {
  auto id = blockIdx.x * blockDim.x + threadIdx.x;
#ifdef TRACE_CUDA
  TRACE_KERNEL(id,0,"kernel_solver");
#endif
  if(id < n) {
    Models::Integrator2DGramian g;
    Models::Integrator2DOptTimeDiff otd;
    Models::Integrator2DOptTimeSolver ots(otd);
    Models::Integrator2DClosedExpm ss_expm;
    Models::Integrator2DCmpClosedExpm cmp_expm;
    // Models::Integrator2DCost cost_fn;
    auto xi = s0[id];
    auto xf = s1[id];
    auto r_mat = *R;
    auto b_mat = *B;
    solve_trajectory<Models::Integrator2DSSComposite::StateType,segment>(xi, xf, ots, g, ss_expm, cmp_expm, r_mat, b_mat, time, out_state, inputs, id);
    // compute_cost(s0[id], s1[id], cost, ots, cost_fn, id);
  }
#ifdef TRACE_CUDA
  TRACE_KERNEL(id,0,"kernel_solver: OK");
#endif
}

template <typename State, typename cost_t, typename time_t>
__global__
void kernel_cost(State *s0, State *s1, time_t *time, cost_t *cost, size_t n) {
  auto id = blockIdx.x * blockDim.x + threadIdx.x;
#ifdef TRACE_CUDA
  TRACE_KERNEL(id,0,"kernel_cost");
#endif
  if(id < n) {
    // Models::Integrator2DGramian g;
    Models::Integrator2DOptTimeDiff otd;
    Models::Integrator2DOptTimeSolver ots(otd);
    // Models::Integrator2DClosedExpm ss_expm;
    // Models::Integrator2DCmpClosedExpm cmp_expm;
    Models::Integrator2DCost cost_fn;
    auto xi = s0[id];
    auto xf = s1[id];
    compute_cost(xi, xf, time, cost, ots, cost_fn, id);
  }
#ifdef TRACE_CUDA
  TRACE_KERNEL(id,0,"kernel_cost : OK");
#endif
}

namespace ModelsGPU {

typedef Models::Integrator2D::State State;
typedef Models::Integrator2D::Input Input;
typedef double Scalar;

struct SolverGPU {
  SolverGPU()
    : dev_cost(DevAllocator<Scalar>(INIT_OBJ_COUNT))
    , dev_time(DevAllocator<Scalar>(INIT_OBJ_COUNT))
    , dev_ftime(DevAllocator<Scalar>(INIT_OBJ_COUNT))
    , dev_inputs(DevAllocator<Input>(INIT_OBJ_COUNT))
    , host_cost(HostAllocator<Scalar>(INIT_OBJ_COUNT))
    , host_time(HostAllocator<Scalar>(INIT_OBJ_COUNT))
    , host_ftime(HostAllocator<Scalar>(INIT_OBJ_COUNT))
    , host_inputs(HostAllocator<Input>(INIT_OBJ_COUNT))
  {
    R = Models::Integrator2DSS::InputWeightType::Identity();
    B << 0, 0, 0, 0, 1, 0, 0, 1;
    gpuErrchk(cudaMalloc(&dev_B,sizeof(Models::Integrator2DSS::InputMatrix)));
    gpuErrchk(cudaMalloc(&dev_R,sizeof(Models::Integrator2DSS::InputWeightType)));
    gpuErrchk(cudaMemcpy(dev_B,&B,sizeof(Models::Integrator2DSS::InputMatrix),cudaMemcpyHostToDevice));
    gpuErrchk(cudaMemcpy(dev_R,&R,sizeof(Models::Integrator2DSS::InputWeightType),cudaMemcpyHostToDevice));
  }

  template<int segment = 10, typename iStateArray, typename fStateArray, typename TrajectoryMapper, typename CostMapper>
  void solve(const iStateArray &xi, const fStateArray &xf, TrajectoryMapper &t_map, CostMapper &c_map)
  {
    // make aliases for state type, deducing from state array
    // and remove reference
    using iStateType = std::decay_t<decltype(xi.at(0))>;
    using fStateType = std::decay_t<decltype(xf.at(0))>;

    auto n = xi.size();
    // CostGPU<Scalar> cost, dev_cost;
    // TrajectoryGPU<Scalar,State,Input> trajectory, dev_trajectory;
    // State *dev_xi, *dev_xf;

    // use static var to avoid excessive mem op
    // the var defined here instead as member var
    // bc of template deduction
    static DevAllocator<iStateType> dev_xi(INIT_OBJ_COUNT);
    static DevAllocator<fStateType> dev_xf(INIT_OBJ_COUNT);
    static DevAllocator<fStateType> dev_states(INIT_OBJ_COUNT);
    static HostAllocator<fStateType> host_states(INIT_OBJ_COUNT);

    // check if we have enough mem befor copying
    if(!dev_xi.ok(n)) dev_xi.resize(n);
    if(!dev_xf.ok(n)) dev_xf.resize(n);
    if(!dev_cost.ok(n)) dev_cost.resize(n);
    if(!dev_ftime.ok(n)) dev_ftime.resize(n);
    if(!dev_time.ok(n*(segment+1))) dev_time.resize(n*(segment+1));
    if(!dev_states.ok(n*(segment+1))) dev_states.resize(n*(segment+1));
    if(!dev_inputs.ok(n*(segment+1))) dev_inputs.resize(n*(segment+1));
    if(!host_cost.ok(n)) host_cost.resize(n);
    if(!host_ftime.ok(n)) host_ftime.resize(n);
    if(!host_time.ok(n*(segment+1))) host_time.resize(n*(segment+1));
    if(!host_states.ok(n*(segment+1))) host_states.resize(n*(segment+1));
    if(!host_inputs.ok(n*(segment+1))) host_inputs.resize(n*(segment+1));

    dev_xi.copy_from(xi.data(), n);
    dev_xf.copy_from(xf.data(), n);

    // gpuErrchk(cudaMalloc(&dev_xi, n*sizeof(State)));
    // gpuErrchk(cudaMalloc(&dev_xf, n*sizeof(State)));

    // gpuErrchk(cudaMemcpy(dev_xi, xi.data(), n*sizeof(State),cudaMemcpyHostToDevice));
    // gpuErrchk(cudaMemcpy(dev_xf, xf.data(), n*sizeof(State),cudaMemcpyHostToDevice));

    // allocate_cost<Scalar>(&cost, &dev_cost, n);
    // allocate_trajectory<Scalar,State,Input>(&trajectory, &dev_trajectory, n, segment+1);

    dim3 blocks;
    dim3 threads;
    threads.x = (n > thread_per_block ? thread_per_block : ceil(float(n)/WARP_SIZE));
    blocks.x = ceil(float(n) / threads.x);
    kernel_solver<segment><<<blocks,threads>>>(dev_xi.ptr, dev_xf.ptr, dev_time.ptr, dev_states.ptr, dev_inputs.ptr, dev_B, dev_R, n);
    // gpuErrchk(cudaThreadSynchronize());
    // gpuErrchk(cudaGetLastError());
    kernel_cost<<<blocks,threads>>>(dev_xi.ptr, dev_xf.ptr, dev_ftime.ptr, dev_cost.ptr, n);
    gpuErrchk(cudaThreadSynchronize());
    gpuErrchk(cudaGetLastError());
    // copy_cost(&cost, &dev_cost, cudaMemcpyDeviceToHost);
    // copy_trajectory(&trajectory, &dev_trajectory, cudaMemcpyDeviceToHost);

    dev_cost.copy_to(host_cost.ptr, n);
    dev_ftime.copy_to(host_ftime.ptr, n);
    dev_time.copy_to(host_time.ptr, n*(segment+1));
    dev_states.copy_to(host_states.ptr, n*(segment+1));
    dev_inputs.copy_to(host_inputs.ptr, n*(segment+1));

    auto t_ptr = host_time.ptr;
    auto s_ptr = host_states.ptr;
    auto i_ptr = host_inputs.ptr;
    auto c_ptr = host_cost.ptr;
    auto ft_ptr = host_ftime.ptr;
    for(size_t i=0; i<n; i++) {
      auto t_begin = t_ptr;
      auto t_end = t_ptr + (segment+1);
      auto s_begin = s_ptr;
      auto s_end = s_ptr + (segment+1);
      auto i_begin = i_ptr;
      auto i_end = i_ptr + (segment+1);
      t_map(t_begin, t_end, s_begin, s_end, i_begin, i_end);
      c_map(c_ptr[i], ft_ptr[i]);
      t_ptr += (segment+1); s_ptr += (segment+1); i_ptr += (segment+1);
    }

    // gpuErrchk(cudaFree(dev_xi));
    // gpuErrchk(cudaFree(dev_xf));
    // free_cost(&cost, &dev_cost);
    // free_trajectory(&trajectory, &dev_trajectory);
  }
  Models::Integrator2DSS::InputWeightType R;
  Models::Integrator2DSS::InputMatrix B;
  Models::Integrator2DSS::InputMatrix *dev_B;
  Models::Integrator2DSS::InputWeightType *dev_R;
  DevAllocator<Scalar> dev_cost;
  DevAllocator<Scalar> dev_time;
  DevAllocator<Scalar> dev_ftime;
  DevAllocator<Input> dev_inputs;
  HostAllocator<Scalar> host_cost;
  HostAllocator<Scalar> host_time;
  HostAllocator<Scalar> host_ftime;
  HostAllocator<Input> host_inputs;

  size_t thread_per_block = THREAD_PER_BLOCK;
};
}

#endif // INTEGRATOR2D_CUHPP
